{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12497486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: Loading the dataset into python\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13f5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_dir = \"dataset/spam\"\n",
    "ham_dir = \"dataset/ham\"\n",
    "\n",
    "emails = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d92372",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'dataset/spam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# loading the emails\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspam_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(spam_dir, filename), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      5\u001b[0m         emails\u001b[38;5;241m.\u001b[39mappend(file\u001b[38;5;241m.\u001b[39mread())\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'dataset/spam'"
     ]
    }
   ],
   "source": [
    "# loading the emails\n",
    "\n",
    "for filename in os.listdir(spam_dir):\n",
    "    with open(os.path.join(spam_dir, filename), 'r', encoding='latin-1') as file:\n",
    "        emails.append(file.read())\n",
    "        labels.append(1) # 1 for spam mail\n",
    "\n",
    "for filename in os.listdir(ham_dir):\n",
    "    with open(os.path.join(ham_dir , filename), 'r', encoding='latin-1') as file:\n",
    "        emails.append(file.read())\n",
    "        labels.append(0) # 0 for ham\n",
    "\n",
    "# here we just appended the mails and labels to their array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538982f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33702, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: fw : this is the solution i mentioned...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: adv : space saving computer to replac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: advs\\ngreetings ,\\ni am benedicta lin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: fw : account over due wfxu ppmfztdtet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: whats new in summer ? bawled\\ncarolyn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Subject: fw : this is the solution i mentioned...      1\n",
       "1  Subject: adv : space saving computer to replac...      1\n",
       "2  Subject: advs\\ngreetings ,\\ni am benedicta lin...      1\n",
       "3  Subject: fw : account over due wfxu ppmfztdtet...      1\n",
       "4  Subject: whats new in summer ? bawled\\ncarolyn...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'text': emails, 'label': labels})\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "# here we made the df and printed the (Rows, coloumnes) and peaked at the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97bc1c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: proccessing the text\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "430cfc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MHOMM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\MHOMM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb4e7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.clear()\n",
    "nltk.data.path.append(os.path.join(os.getenv('APPDATA'), 'nltk_data'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22920b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_email(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()      # remove HTML\n",
    "    text = text.lower()                                       # lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|[^a-z\\s]', '', text)       # remove URLs & punctuation\n",
    "    words = word_tokenize(text)                          # tokenize\n",
    "    words = [stemmer.stem(w) for w in words if w not in stop_words]  # stem + remove stopwords\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee334b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileSystemPathPointer('C:\\\\Users\\\\MHOMM\\\\AppData\\\\Roaming\\\\nltk_data\\\\tokenizers\\\\punkt')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.find('tokenizers/punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05a8559c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MHOMM\\AppData\\Local\\Temp\\ipykernel_20140\\918289155.py:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()      # remove HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: fw : this is the solution i mentioned...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject fw solut mention lsc oo thank email ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: adv : space saving computer to replac...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject adv space save comput replac big box d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: advs\\ngreetings ,\\ni am benedicta lin...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject adv greet benedicta lindiw hendrick mr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: fw : account over due wfxu ppmfztdtet...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject fw account due wfxu ppmfztdtet elimin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: whats new in summer ? bawled\\ncarolyn...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject what new summer bawl carolyn regret wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Subject: fw : this is the solution i mentioned...      1   \n",
       "1  Subject: adv : space saving computer to replac...      1   \n",
       "2  Subject: advs\\ngreetings ,\\ni am benedicta lin...      1   \n",
       "3  Subject: fw : account over due wfxu ppmfztdtet...      1   \n",
       "4  Subject: whats new in summer ? bawled\\ncarolyn...      1   \n",
       "\n",
       "                                          clean_text  \n",
       "0  subject fw solut mention lsc oo thank email ad...  \n",
       "1  subject adv space save comput replac big box d...  \n",
       "2  subject adv greet benedicta lindiw hendrick mr...  \n",
       "3  subject fw account due wfxu ppmfztdtet elimin ...  \n",
       "4  subject what new summer bawl carolyn regret wa...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['text'].apply(clean_email)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05786a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feature Extraction (Vectorize)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0df1f735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (33702, 3000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "X = vectorizer.fit_transform(df['clean_text']).toarray()\n",
    "y = df['label']\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0712e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Trainning the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3bb31ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model training completed!\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"âœ… Model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6fb2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: evaluating the model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf6b890a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Evaluation Metrics:\n",
      "Accuracy: 0.9780\n",
      "Precision: 0.9716\n",
      "Recall: 0.9857\n",
      "F1 Score: 0.9786\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3207   99]\n",
      " [  49 3386]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"ðŸ“ˆ Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82313ee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 5\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mmodel\u001b[49m, f)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorizer.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(vectorizer, f)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Saving the model and vectorizer\n",
    "import pickle\n",
    "\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "print(\"âœ… Model and vectorizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e165e032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
